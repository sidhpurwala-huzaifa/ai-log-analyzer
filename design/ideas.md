**AI-Powered Log Triage and Security Alert Aggregator for Fedora**

**Author:** Nikhil Jangid  
**Mentor:** Huzaifa Sidhpurwala

**1\. Overview & Motivation**

**Objective:**  
Develop an AI-powered tool that automatically collects, parses, classifies, and prioritizes Fedora security logs. The tool aims to assist system administrators by rapidly identifying critical security events from the vast and diverse log data generated by Fedora systems.

**Motivation:**

- **Reduce Manual Effort:** Fedora systems produce a high volume of logs (e.g., SELinux, systemd journal, audit logs). Manually reviewing these logs is both time-consuming and error-prone.
- **Improve Security Response:** By automatically triaging and flagging important security events, the tool helps administrators focus on incidents that require immediate action.
- **Enhance System Reliability:** Providing real-time insights into system behavior can help with troubleshooting, compliance, and overall system performance monitoring.

**2\. Log Types & Data Sources**

**A. SELinux Logs**

- **Description:**  
    Generated by SELinux, these logs capture events such as policy violations and access control errors.
- **Generation & Storage:**
  - Integrated into the Linux audit system, they are typically stored in /var/log/audit/audit.log or within broader system log files.
  - They are generated continuously by the SELinux enforcement mechanism.
- **Retrieval:**
  - Tools such as ausearch or sealert are used to filter and view SELinux events.
  - Our approach will extract these logs and convert them into a structured format (e.g., JSON) for further processing.

**B. Systemd Journal Logs**

- **Description:**  
    These logs encompass messages from the kernel, system services, and various applications managed by systemd.
- **Generation & Storage:**
  - Stored in a binary format under /var/log/journal/.
  - They can be exported in JSON format using the command:

journalctl -o json > systemd_logs.json

- **Retrieval:**
  - The JSON format enables straightforward parsing, allowing us to extract fields like timestamps, service names, and message content.

**C. Audit Logs**

- **Description:**  
    Produced by the Linux Audit subsystem, these logs record detailed information about system calls, file accesses, and other security-related events.
- **Generation & Storage:**
  - Typically found in /var/log/audit/audit.log.
  - They provide a forensic trail for security analysis.
- **Retrieval:**
  - Using utilities such as ausearch, logs can be filtered and exported for processing.

**Note:**  
My current understanding is based on general online documentation. If there are Fedora-specific datasets or configurations (e.g., custom audit rules or log file locations) that should be considered, I welcome further guidance.

**3\. Data Extraction & Preprocessing**

**Extraction:**

- **Log Collection:**
  - Develop Python scripts that utilize command-line tools like journalctl and ausearch to extract raw logs.
  - Convert the raw outputs to structured formats (JSON or CSV), which makes downstream processing easier.

**Parsing & Normalization:**

- **Parsing:**
  - Write custom parsers in Python using regular expressions or specialized log-parsing libraries.
  - Extract key fields such as timestamp, severity, log source, event type, and message content.
- **Normalization:**
  - Standardize date formats, severity levels, and other identifiers.
  - Clean log messages by removing redundant whitespace and non-informative tokens.
- **Output:**
  - Generate structured datasets that can serve as inputs for our AI/NLP pipeline.

**4\. AI/NLP Model Approach**

**Available Pre-Built Models:**

- **Mistral-7B-Instruct-v0.2-GGUF:**
  - **Usage:**
    - As used in Log Detective, this model is available in gguf format via llama-cpp-python.
    - It is optimized for log analysis and can handle summarization tasks.
- **Meta-Llama-3-8B-Instruct-GGUF:**
  - **Alternative:**
    - This model serves as an alternative if the default does not meet performance or compatibility requirements.
  - **Access:**
    - Available via direct URL download or local storage if already present.

**Summarization & Template Mining:**

- **Drain Template Miner:**
  - Utilized for clustering similar log messages.
  - Helps reduce the token count by summarizing repetitive log patterns.

**Custom Model Development (If Needed):**

- **Rationale:**
  - In case the pre-built models do not capture Fedora-specific nuances or if further performance tuning is required.
- **Approaches:**
  - **Traditional ML Methods:**
    - Use scikit-learn to build classifiers based on engineered features (TF-IDF, word counts, etc.).
  - **Deep Learning Approaches:**
    - Utilize frameworks such as PyTorch or TensorFlow to fine-tune a language model on a curated dataset of Fedora logs.
- **Model Pipeline:**
  - **Feature Extraction:** Tokenize log messages and generate numerical representations using methods like TF-IDF or word embeddings.
  - **Classification:** Use the selected model (pre-built or custom) to label logs as “normal” or “requires immediate action.”
  - **Aggregation:** Cluster similar log entries (using Drain or clustering algorithms) to produce a concise report of critical alerts.

**5\. System Architecture & Workflow**

**Modules:**

1. **Log Ingestion Module:**
    - Collects logs from SELinux, systemd journal, and audit sources.
    - Uses command-line utilities and Python scripts.
2. **Preprocessing Module:**
    - Parses, cleans, and normalizes raw log data.
    - Outputs structured data (JSON/CSV) for the AI pipeline.
3. **AI/NLP Inference Module:**
    - Processes structured log data through selected models (Mistral-7B-Instruct-v0.2-GGUF, Meta-Llama-3-8B-Instruct-GGUF, and/or custom models).
    - Performs classification and summarization using LLM and Drain template miner.
4. **Aggregation & Reporting Module:**
    - Groups similar alerts and generates prioritized reports.
    - Provides output via a CLI, simple web dashboard, or API.
5. **Packaging:**
    - The final tool will be packaged as a Fedora-compliant RPM, ensuring easy deployment and integration.

**6\. Implementation Timeline (Tentative)**

- **Weeks 1-2: Research & Setup**
  - Gather detailed Fedora log documentation and set up development environment.
  - Prototype log collection for one source (e.g., SELinux logs).
- **Weeks 3-4: Log Extraction & Preprocessing**
  - Develop parsers and normalization routines.
  - Convert raw logs into structured JSON/CSV formats.
- **Weeks 5-6: AI/NLP Integration**
  - Integrate pre-built models (Mistral-7B-Instruct-v0.2-GGUF and alternative).
  - Experiment with Drain template miner for summarization.
- **Weeks 7-8: Model Tuning & Customization**
  - Evaluate model performance and consider custom model training if necessary.
  - Develop the classification pipeline and adjust feature extraction.
- **Weeks 9-10: Aggregation & Reporting**
  - Build the aggregation module to group and prioritize alerts.
  - Create a simple CLI or dashboard for real-time monitoring.
- **Weeks 11-12: Packaging & Final Testing**
  - Package the application as a Fedora-compliant RPM.
  - Finalize documentation and conduct performance benchmarking.

**7\. Conclusion**

This design document outlines a comprehensive approach to building an AI-powered log triage and security alert aggregator for Fedora. By leveraging a modular architecture that includes log ingestion, preprocessing, AI/NLP inference, and aggregation, the tool aims to reduce noise and enhance security monitoring. The initial focus will be on integrating pre-built models (e.g., Mistral-7B-Instruct-v0.2-GGUF) and Drain for summarization, with the possibility of custom model development as needed. Packaging the solution as an RPM will ensure easy deployment and adoption in Fedora environments.
